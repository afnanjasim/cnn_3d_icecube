{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Ice-Cube 3D CNN\n",
    "Using Wahid's tutorial keras code on ICE-Cube CNN data.\n",
    "- October 5, 2018: 2D CNN works.\n",
    "- October 6, 2018: got simple 3D CNN to work.\n",
    "- Oct 10, 2018: Changes made:\n",
    "    - Using cross validation and shuffling arguments of the fit function.\n",
    "- Oct 16, 2018: Reading in actual data, which is very large. Saving to files. Split code into 2 parts: process data and train. Putting back shuffle and test options.\n",
    "- Oct 18, 2018: This notebook does the entire workflow (except processing raw data) for 4 different models, in modular way, without classes.\n",
    "The data formatting is still not modular.\n",
    "- Oct 22, 2018: Saving model and history to project space. history files are 12GB! Just saving history.history to save space.\n",
    "- Oct 24, 2018: Adding weights to testing process.\n",
    "- Oct 29, 2018: Saving predicted values now, for quick plotting. Train-cv on regular data, Testing on reserved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful blog for keras conv3D: http://learnandshare645.blogspot.com/2016/06/3d-cnn-in-keras-action-recognition.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# keras modules\n",
    "import keras\n",
    "from keras import layers, models, optimizers, callbacks  # or tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "- Data processing\n",
    "    - Read raw data\n",
    "    - Process data\n",
    "    - Save process data\n",
    "    - Read processed data\n",
    "- Model\n",
    "    - Define model\n",
    "    - Train model\n",
    "    - Validate model\n",
    "    - Plot accuracy and loss\n",
    "    - Save model\n",
    "- Test\n",
    "    - Read model\n",
    "    - Read training data\n",
    "    - Get weights\n",
    "    - Test model\n",
    "    - Plot ROC curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data \n",
    "#### Get data from the .hdf5 files. This takes a long time. Do this only once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this is in bash\n",
    "\n",
    "\n",
    "/global/project/projectdirs/dasrepo/vpa/ice_cube/data_for_cnn/extracted_data_v/scripts/run_extract.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from files\n",
    "def f_load_data(data_dir,f1,f2,f3):\n",
    "    ''' Load extracted data from files. Three files for xdata,ydata,weights.\n",
    "    arguments: data directory, f1,f2,f3 \n",
    "    returns : inpx,inpy,weights as arrays\n",
    "    '''\n",
    "\n",
    "    inpx=np.load(data_dir+f1+'.npy')\n",
    "    inpy=np.load(data_dir+f2+'.npy')\n",
    "    wts=np.load(data_dir+f3+'.npy')\n",
    "    print(inpx.shape,inpy.shape)\n",
    "    \n",
    "    \n",
    "    return inpx,inpy,wts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Format data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Shuffle and split data ####\n",
    "\n",
    "def f_shuffle_data(inpx,inpy,wts):\n",
    "    ## Shuffle data\n",
    "    \n",
    "    # Setting seed\n",
    "    seed=243\n",
    "    np.random.seed(seed=seed)\n",
    "\n",
    "    ## Get shuffled array of indices\n",
    "    shuffle_arr=np.arange(inpx.shape[0])\n",
    "    np.random.shuffle(shuffle_arr)\n",
    "    inpx=inpx[shuffle_arr]\n",
    "    inpy=inpy[shuffle_arr]\n",
    "    wts=wts[shuffle_arr]\n",
    "\n",
    "    return inpx,inpy,wts\n",
    "\n",
    "def f_drop_data(inpx,inpy,wts,data_size):\n",
    "    # Drop data for quick training. Just taking the slice of the data from the top.\n",
    "    \n",
    "    full_size=inpy.shape[0]\n",
    "    assert(data_size<=full_size),\"data_size: %s in f_drop_data is more than full data size: %s\"%(data_size,full_size)\n",
    "        \n",
    "    temp=inpx[:data_size]\n",
    "    del(inpx)\n",
    "    inpx=temp.copy()\n",
    "    temp=inpy[:data_size]\n",
    "    del(inpy)\n",
    "    inpy=temp.copy()\n",
    "    temp=wts[:data_size]\n",
    "    del(wts)\n",
    "    wts=temp.copy()\n",
    "    \n",
    "    del(temp)    \n",
    "    \n",
    "    \n",
    "    return (inpx,inpy,wts)        \n",
    "        \n",
    "       \n",
    "\n",
    "def f_split_data(inpx,inpy,wts,test_fraction):\n",
    "    '''\n",
    "    Split data for training and test. validation from training piece of data.\n",
    "    !! Warning this code deletes inpx,inpy inside the function. can't help it because the arrays are too big!!\n",
    "    '''\n",
    "    \n",
    "    num=inpx.shape[0]\n",
    "    test_idx=int(test_fraction*num)\n",
    "    train_idx=num-test_idx\n",
    "\n",
    "    train_x=inpx[:train_idx]\n",
    "    train_y=inpy[:train_idx]\n",
    "    train_wts=wts[:train_idx]\n",
    "    \n",
    "    test_x=inpx[train_idx:]\n",
    "    test_y=inpy[train_idx:]\n",
    "    test_wts=wts[train_idx:]\n",
    "    \n",
    "    return train_x,train_y,train_wts,test_x,test_y,test_wts\n",
    "\n",
    "\n",
    "def f_format_data(inpx,inpy,wts,shuffle_flag=True,drop_data=True,data_size=1000,test_fraction=0.25):\n",
    "    ''' Shuffle, drop and split data for train-test\n",
    "    '''\n",
    "    # Shuffle data\n",
    "    if shuffle_flag: inpx,inpy,wts=f_shuffle_data(inpx,inpy,wts)\n",
    "    # Drop data\n",
    "    if drop_data: inpx,inpy,wts=f_drop_data(inpx,inpy,wts,data_size)\n",
    "\n",
    "#     print(inpy[inpy==0.0].shape,inpy[inpy>0.0].shape,inpy.shape)\n",
    "    \n",
    "#     # Plot data\n",
    "#     plt.figure()\n",
    "#     plt.plot(inpy[:],linestyle='',marker='*',markersize=1)\n",
    "#     plt.title(\"Plot of y data after shuffle\")\n",
    "#     plt.show() \n",
    "    \n",
    "    # Split data into train-test.\n",
    "    train_x,train_y,train_wts,test_x,test_y,test_wts=f_split_data(inpx,inpy,wts,test_fraction)\n",
    "    \n",
    "    print('Data sizes: train_x{0},train_y{1},test_x{2},test_y{3}'.format(train_x.shape,train_y.shape,test_x.shape,test_y.shape))\n",
    "\n",
    "    return train_x,train_y,train_wts,test_x,test_y,test_wts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining all the models tried in the study\n",
    "\n",
    "def f_define_model(inpx,name):\n",
    "    '''\n",
    "    Function that defines the model and compiles it.\n",
    "    '''\n",
    "    \n",
    "    inputs = layers.Input(shape=inpx.shape[1:])\n",
    "    h = inputs\n",
    "    \n",
    "    # Choose model\n",
    "    if name=='1':\n",
    "        print(\"model %s\"%name)\n",
    "        # Convolutional layers\n",
    "        conv_sizes=[10, 10, 10]\n",
    "        conv_args = dict(kernel_size=(3, 3, 3), activation='relu', padding='same')\n",
    "        for conv_size in conv_sizes:\n",
    "            h = layers.Conv3D(conv_size, **conv_args)(h)\n",
    "            h = layers.MaxPooling3D(pool_size=(2, 2, 2))(h)\n",
    "    #         h = layers.Dropout(0.5)(h)\n",
    "        h = layers.Flatten()(h)\n",
    "\n",
    "        # Fully connected  layers\n",
    "        h = layers.Dense(10, activation='relu')(h)\n",
    "        #    h = layers.Dropout(0.5)(h)\n",
    "\n",
    "        # Ouptut layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(h)\n",
    "    \n",
    "    \n",
    "    elif name=='2':\n",
    "        print(\"model %s\"%name)\n",
    "        # Convolutional layers\n",
    "        conv_sizes=[10,10,10]\n",
    "        conv_args = dict(kernel_size=(3, 3, 3), activation='relu', padding='same')\n",
    "        for conv_size in conv_sizes:\n",
    "            h = layers.Conv3D(conv_size, **conv_args)(h)\n",
    "            h = layers.MaxPooling3D(pool_size=(2, 2, 2))(h)\n",
    "            h = layers.Dropout(0.5)(h)\n",
    "        h = layers.Flatten()(h)\n",
    "\n",
    "        # Fully connected  layers\n",
    "        h = layers.Dense(64, activation='relu')(h)\n",
    "        h = layers.Dropout(0.5)(h)\n",
    "\n",
    "        # Ouptut layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(h)\n",
    "        \n",
    "    elif name=='3':\n",
    "        print(\"model %s\"%name)\n",
    "        # Convolutional layers\n",
    "        conv_sizes=[6,6,6]\n",
    "        conv_args = dict(kernel_size=(3, 3, 3), activation='relu', padding='same')\n",
    "        for conv_size in conv_sizes:\n",
    "            h = layers.Conv3D(conv_size, **conv_args)(h)\n",
    "            h = layers.MaxPooling3D(pool_size=(2, 2, 2))(h)\n",
    "            h = layers.Dropout(0.5)(h)\n",
    "        h = layers.Flatten()(h)\n",
    "\n",
    "        # Fully connected  layers\n",
    "        h = layers.Dense(64, activation='relu')(h)\n",
    "        h = layers.Dropout(0.5)(h)\n",
    "\n",
    "        # Ouptut layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(h)\n",
    "    \n",
    "    elif name=='4':\n",
    "        print(\"model %s\"%name)\n",
    "        # Convolutional layers\n",
    "        conv_sizes=[6,6,6]\n",
    "        conv_args = dict(kernel_size=(3, 3, 3), activation='relu', padding='same')\n",
    "        for conv_size in conv_sizes:\n",
    "            h = layers.Conv3D(conv_size, **conv_args)(h)\n",
    "            h = layers.MaxPooling3D(pool_size=(2, 2, 2))(h)\n",
    "            h = layers.Dropout(0.5)(h)\n",
    "        h = layers.Flatten()(h)\n",
    "\n",
    "        # Fully connected  layers\n",
    "        h = layers.Dense(120, activation='relu')(h)\n",
    "        h = layers.Dropout(0.5)(h)\n",
    "\n",
    "        # Ouptut layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(h)\n",
    "        \n",
    "    elif name=='5':\n",
    "        print(\"model %s\"%name)\n",
    "        # Convolutional layers\n",
    "        conv_sizes=[6,6]\n",
    "        conv_args = dict(kernel_size=(2, 4, 15), activation='relu', padding='same')\n",
    "        for conv_size in conv_sizes:\n",
    "            h = layers.Conv3D(conv_size, **conv_args)(h)\n",
    "            h = layers.MaxPooling3D(pool_size=(3, 3, 3))(h)\n",
    "            h = layers.Dropout(0.5)(h)\n",
    "        h = layers.Flatten()(h)\n",
    "\n",
    "        # Fully connected  layers\n",
    "        h = layers.Dense(120, activation='relu')(h)\n",
    "        h = layers.Dropout(0.5)(h)\n",
    "\n",
    "        # Ouptut layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(h)\n",
    "        \n",
    "    elif name=='resnet50':\n",
    "        model=keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_tensor=inputs, input_shape=inpx.shape[1:], pooling=max, classes=1000)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    ############################################\n",
    "    ####### Compile model ######################\n",
    "    ############################################\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and perform fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_train_model(model,inpx,inpy,num_epochs=5):\n",
    "    '''\n",
    "    Train model. Returns just history.history\n",
    "    '''\n",
    "    cv_fraction=0.33 # Fraction of data for cross validation\n",
    "    \n",
    "    history=model.fit(x=inpx, y=inpy,\n",
    "                    batch_size=32,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=1,\n",
    "#                     callbacks = [callbacks.ModelCheckpoint('./rpv_weights.h5')],\n",
    "                    validation_split=cv_fraction,\n",
    "                    shuffle=True\n",
    "                )\n",
    "    \n",
    "    print(\"Number of parameters\",model.count_params())\n",
    "    \n",
    "    return history.history\n",
    "\n",
    "def f_plot_learning(history):\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    # Plot training & validation accuracy values\n",
    "    fig.add_subplot(2,1,1)\n",
    "    plt.plot(history['acc'],label='Train')\n",
    "    plt.plot(history['val_acc'],label='Validation')\n",
    "#     plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    # Plot loss values\n",
    "    fig.add_subplot(2,1,2)\n",
    "    plt.plot(history['loss'],label='Train')\n",
    "    plt.plot(history['val_loss'],label='Validation')\n",
    "#     plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def f_plot_roc_curve(fpr,tpr):\n",
    "    '''\n",
    "    Module for roc plot and printing AUC\n",
    "    '''\n",
    "    plt.figure()\n",
    "    # plt.plot(fpr,tpr)\n",
    "    plt.scatter(fpr,tpr)\n",
    "    plt.xscale('log')\n",
    "    plt.show()\n",
    "\n",
    "    # AUC \n",
    "    auc_val = auc(fpr, tpr)\n",
    "    print(\"AUC: \",auc_val)\n",
    "    \n",
    "\n",
    "def f_test_model(xdata,ydata,wts,model,model_name,model_save_dir,test_status=False):\n",
    "    '''\n",
    "    Test model and make ROC plot\n",
    "    If model has been tested, store the y-predict values\n",
    "    and read them in next time.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    test_file_name=model_save_dir+'y-predict_model-'+str(model_name)+'.pred'\n",
    "    \n",
    "    \n",
    "#     model.evaluate(xdata,ydata,sample_weights=wts,verbose=1)\n",
    "    if not test_status:# Predict values and store to file.\n",
    "        y_pred=model.predict(xdata,verbose=1)\n",
    "        # Save prediction file\n",
    "        np.savetxt(test_file_name,y_pred)\n",
    "        \n",
    "    else: # Load y_predictions from file.\n",
    "        print(\"Using test prediction from previous test\",test_file_name)\n",
    "        y_pred=np.loadtxt(test_file_name)\n",
    "        \n",
    "#     print(y_pred)\n",
    "    fpr,tpr,threshold=roc_curve(ydata,y_pred,sample_weight=wts)\n",
    "    print(fpr.shape,tpr.shape,threshold.shape)\n",
    "    f_plot_roc_curve(fpr,tpr)\n",
    "\n",
    "\n",
    "def f_perform_fit(train_x,train_y,train_wts,test_x,test_y,test_wts,model_dict,num_epochs,train_status=False,test_status=False):\n",
    "    '''\n",
    "    Compile, train, save and test the model.\n",
    "    Steps:\n",
    "    - Compile\n",
    "    - Train\n",
    "    - Save\n",
    "    - Read\n",
    "    - Plot\n",
    "    - Test\n",
    "    \n",
    "    Note: Cross-validation data is built into the training. So, train_{x/y} contains the training and cval data.\n",
    "    '''\n",
    "    \n",
    "    model_save_dir='/global/project/projectdirs/dasrepo/vpa/ice_cube/data_for_cnn/saved_models/'\n",
    "    model_name=model_dict['name'] # string for the model\n",
    "    fname_model,fname_history='model_{0}.h5'.format(model_name),'history_{0}.pickle'.format(model_name)\n",
    "    \n",
    "    if not train_status: # If not trained before, train the model and save it.\n",
    "\n",
    "        ########################\n",
    "        # Compile model\n",
    "        model=f_define_model(train_x,model_name)\n",
    "        # Train model\n",
    "        history=f_train_model(model,train_x,train_y,num_epochs)\n",
    "\n",
    "        ########################\n",
    "        # Save model and history\n",
    "        model.save(model_save_dir+fname_model)\n",
    "        with open(model_save_dir+fname_history, 'wb') as f:\n",
    "                pickle.dump(history, f)\n",
    "    \n",
    "    else:\n",
    "        print(\"Using trained model\")\n",
    "\n",
    "        \n",
    "    ########################\n",
    "    ### Read model and history\n",
    "    \n",
    "    ### Check if files exist\n",
    "    assert os.path.exists(model_save_dir+fname_model),\"Model not saved\"\n",
    "    assert os.path.exists(model_save_dir+fname_history),\"History not saved\"\n",
    "    \n",
    "    model=load_model(model_save_dir+fname_model)\n",
    "    with open(model_save_dir+fname_history,'rb') as f:\n",
    "        history= pickle.load(f)\n",
    "    \n",
    "    ########################\n",
    "    model.summary()\n",
    "    # Plot tested model\n",
    "    f_plot_learning(history)\n",
    "    \n",
    "    ########################\n",
    "    # Test model\n",
    "    f_test_model(test_x,test_y,test_wts,model,model_dict['name'],model_save_dir,test_status)\n",
    "\n",
    "    model_dict['model'],model_dict['history']=model,history\n",
    "    \n",
    "    return model_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_status=True\n",
    "# train_status=False\n",
    "# test_status=False\n",
    "test_status=True\n",
    "\n",
    "\n",
    "model_dict={'name':'1','description':'simplest','model':None,'history':None}\n",
    "model_dict1=f_perform_fit(train_x,train_y,train_wts,test_x,test_y,test_wts,model_dict,num_epochs=5,train_status,test_status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136066, 10, 20, 60, 1) (136066,)\n",
      "Data sizes: train_x(102050, 10, 20, 60, 1),train_y(102050,),test_x(34016, 10, 20, 60, 1),test_y(34016,)\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    ###Extract data\n",
    "    data_dir='/global/project/projectdirs/dasrepo/vpa/ice_cube/data_for_cnn/extracted_data_v/data/'\n",
    "    f1,f2,f3='processed_input_regular_x','processed_input_regular_y','processed_input_regular_wts'\n",
    "    inpx,inpy,wts=f_load_data(data_dir,f1,f2,f3)\n",
    "    # print(sys.getsizeof(inpx))\n",
    "\n",
    "    ###Format data\n",
    "    ### cross-validation done with part of train data.\n",
    "    train_x,train_y,train_wts,test_x,test_y,test_wts=f_format_data(inpx,inpy,wts,shuffle_flag=True,drop_data=False,data_size=10000,test_fraction=0.25)\n",
    "\n",
    "    del(inpx,inpy,wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`input_shape` must be a tuple of three integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-58695bc1204e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'resnet50'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'simplest'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'history'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel_dict1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_perform_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_wts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_wts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_status\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-aad83d4230b4>\u001b[0m in \u001b[0;36mf_perform_fit\u001b[0;34m(train_x, train_y, train_wts, test_x, test_y, test_wts, model_dict, num_epochs, train_status, test_status)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Compile model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_define_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-28d7e2aa4c20>\u001b[0m in \u001b[0;36mf_define_model\u001b[0;34m(inpx, name)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'resnet50'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/v_py3/lib/python3.6/site-packages/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/v_py3/lib/python3.6/site-packages/keras/applications/resnet50.py\u001b[0m in \u001b[0;36mResNet50\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/v_py3/lib/python3.6/site-packages/keras_applications/resnet50.py\u001b[0m in \u001b[0;36mResNet50\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                                       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                                       \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                                       weights=weights)\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/v_py3/lib/python3.6/site-packages/keras_applications/imagenet_utils.py\u001b[0m in \u001b[0;36m_obtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 313\u001b[0;31m                         '`input_shape` must be a tuple of three integers.')\n\u001b[0m\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                     raise ValueError('The input must have 3 channels; got '\n",
      "\u001b[0;31mValueError\u001b[0m: `input_shape` must be a tuple of three integers."
     ]
    }
   ],
   "source": [
    "# train_status=True\n",
    "train_status=False\n",
    "test_status=False\n",
    "# test_status=True\n",
    "\n",
    "\n",
    "model_dict={'name':'resnet50','description':'simplest','model':None,'history':None}\n",
    "model_dict1=f_perform_fit(train_x,train_y,train_wts,test_x,test_y,test_wts,model_dict,num_epochs=5,train_status=train_status,test_status=test_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 2\n",
      "Train on 68373 samples, validate on 33677 samples\n",
      "Epoch 1/5\n",
      "68373/68373 [==============================] - 427s 6ms/step - loss: 1.2012 - acc: 0.7939 - val_loss: 0.4874 - val_acc: 0.8059\n",
      "Epoch 2/5\n",
      "68373/68373 [==============================] - 432s 6ms/step - loss: 0.4733 - acc: 0.8139 - val_loss: 0.4864 - val_acc: 0.8064\n",
      "Epoch 3/5\n",
      "68373/68373 [==============================] - 426s 6ms/step - loss: 0.4457 - acc: 0.8226 - val_loss: 0.4683 - val_acc: 0.8077\n",
      "Epoch 4/5\n",
      "68373/68373 [==============================] - 410s 6ms/step - loss: 0.4177 - acc: 0.8341 - val_loss: 0.4411 - val_acc: 0.8115\n",
      "Epoch 5/5\n",
      "68373/68373 [==============================] - 408s 6ms/step - loss: 0.3937 - acc: 0.8464 - val_loss: 0.4181 - val_acc: 0.8176\n",
      "Number of parameters 14789\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10, 20, 60, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 10, 20, 60, 10)    280       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 5, 10, 30, 10)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 10, 30, 10)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 5, 10, 30, 10)     2710      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 2, 5, 15, 10)      0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 5, 15, 10)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 2, 5, 15, 10)      2710      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 1, 2, 7, 10)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 2, 7, 10)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 140)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                9024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 14,789\n",
      "Trainable params: 14,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542c588714404ff89582a5b3e10f18ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8059266aec844ba9ca5889653e34a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34016/34016 [==============================] - 77s 2ms/step\n",
      "(33911,) (33911,) (33911,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cd639bd16544ae9da7061be23da575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.842717218514279\n"
     ]
    }
   ],
   "source": [
    "# train_status=True\n",
    "train_status=False\n",
    "test_status=False\n",
    "# test_status=True\n",
    "\n",
    "model_dict={'name':'2','description':'simplest','model':None,'history':None}\n",
    "model_dict2=f_perform_fit(train_x,train_y,train_wts,test_x,test_y,test_wts,model_dict,num_epochs=5,train_status,test_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 3\n",
      "Train on 68373 samples, validate on 33677 samples\n",
      "Epoch 1/5\n",
      "68373/68373 [==============================] - 257s 4ms/step - loss: 1.3657 - acc: 0.7929 - val_loss: 0.4903 - val_acc: 0.8057\n",
      "Epoch 2/5\n",
      "68373/68373 [==============================] - 260s 4ms/step - loss: 0.4802 - acc: 0.8090 - val_loss: 0.4827 - val_acc: 0.8060\n",
      "Epoch 3/5\n",
      "68373/68373 [==============================] - 264s 4ms/step - loss: 0.4656 - acc: 0.8130 - val_loss: 0.4758 - val_acc: 0.8065\n",
      "Epoch 4/5\n",
      "68373/68373 [==============================] - 248s 4ms/step - loss: 0.4438 - acc: 0.8212 - val_loss: 0.4602 - val_acc: 0.8089\n",
      "Epoch 5/5\n",
      "68373/68373 [==============================] - 262s 4ms/step - loss: 0.4067 - acc: 0.8360 - val_loss: 0.4425 - val_acc: 0.8134\n",
      "Number of parameters 7629\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 10, 20, 60, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 10, 20, 60, 6)     168       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 5, 10, 30, 6)      0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5, 10, 30, 6)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 5, 10, 30, 6)      978       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 2, 5, 15, 6)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2, 5, 15, 6)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 2, 5, 15, 6)       978       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 1, 2, 7, 6)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1, 2, 7, 6)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                5440      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 7,629\n",
      "Trainable params: 7,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332fe17a225a460cb2be026323f66e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8561361bc8ab46d091409a0b82048cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34016/34016 [==============================] - 55s 2ms/step\n",
      "(33888,) (33888,) (33888,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffd1a19bb0b49248d66064c4db8b145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8645203695240572\n"
     ]
    }
   ],
   "source": [
    "# train_status=True\n",
    "train_status=False\n",
    "test_status=False\n",
    "# test_status=True\n",
    "\n",
    "model_dict={'name':'3','description':'6conv size, extra dropout','model':None,'history':None}\n",
    "model_dict3=f_perform_fit(train_x,train_y,train_wts,test_x,test_y,test_wts,model_dict,num_epochs=5,train_status,test_status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 4\n",
      "Train on 68373 samples, validate on 33677 samples\n",
      "Epoch 1/5\n",
      "68373/68373 [==============================] - 268s 4ms/step - loss: 1.3012 - acc: 0.7950 - val_loss: 0.4926 - val_acc: 0.8057\n",
      "Epoch 2/5\n",
      "68373/68373 [==============================] - 245s 4ms/step - loss: 0.4812 - acc: 0.8102 - val_loss: 0.4772 - val_acc: 0.8065\n",
      "Epoch 3/5\n",
      "68373/68373 [==============================] - 248s 4ms/step - loss: 0.4420 - acc: 0.8204 - val_loss: 0.3991 - val_acc: 0.8149\n",
      "Epoch 4/5\n",
      "68373/68373 [==============================] - 259s 4ms/step - loss: 0.3957 - acc: 0.8365 - val_loss: 0.3577 - val_acc: 0.8298\n",
      "Epoch 5/5\n",
      "68373/68373 [==============================] - 262s 4ms/step - loss: 0.3735 - acc: 0.8464 - val_loss: 0.3495 - val_acc: 0.8316\n",
      "Number of parameters 12445\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 10, 20, 60, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 10, 20, 60, 6)     168       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 5, 10, 30, 6)      0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 5, 10, 30, 6)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 5, 10, 30, 6)      978       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 2, 5, 15, 6)       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2, 5, 15, 6)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 2, 5, 15, 6)       978       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 1, 2, 7, 6)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1, 2, 7, 6)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 120)               10200     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 12,445\n",
      "Trainable params: 12,445\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b3863a265b40cd91037143b4801c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0942933c0c44988e305638521e545a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34016/34016 [==============================] - 53s 2ms/step\n",
      "(33315,) (33315,) (33315,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02853220f1754aa68ae9832ae66e2ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8858907324834038\n"
     ]
    }
   ],
   "source": [
    "# train_status=True\n",
    "train_status=False\n",
    "test_status=False\n",
    "# test_status=True\n",
    "\n",
    "model_dict={'name':'4','description':'6conv size, extra dropout','model':None,'history':None}\n",
    "model_dict4=f_perform_fit(train_x,train_y,train_wts,test_x,test_y,test_wts,model_dict,num_epochs=5,train_status,test_status)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 5\n",
      "Train on 68373 samples, validate on 33677 samples\n",
      "Epoch 1/5\n",
      "68373/68373 [==============================] - 550s 8ms/step - loss: 3.0213 - acc: 0.8055 - val_loss: 2.9154 - val_acc: 0.8065\n",
      "Epoch 2/5\n",
      "68373/68373 [==============================] - 550s 8ms/step - loss: 0.9137 - acc: 0.8043 - val_loss: 0.4864 - val_acc: 0.8069\n",
      "Epoch 3/5\n",
      "68373/68373 [==============================] - 560s 8ms/step - loss: 0.4873 - acc: 0.8084 - val_loss: 0.4835 - val_acc: 0.8067\n",
      "Epoch 4/5\n",
      "68373/68373 [==============================] - 567s 8ms/step - loss: 0.4853 - acc: 0.8085 - val_loss: 0.4810 - val_acc: 0.8061\n",
      "Epoch 5/5\n",
      "68373/68373 [==============================] - 555s 8ms/step - loss: 0.4801 - acc: 0.8092 - val_loss: 0.4812 - val_acc: 0.8061\n",
      "Number of parameters 13933\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 10, 20, 60, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_13 (Conv3D)           (None, 10, 20, 60, 6)     726       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_13 (MaxPooling (None, 3, 6, 20, 6)       0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 3, 6, 20, 6)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 3, 6, 20, 6)       4326      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_14 (MaxPooling (None, 1, 2, 6, 6)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1, 2, 6, 6)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 120)               8760      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 13,933\n",
      "Trainable params: 13,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe70d2a33504933a085679ec029e3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d370bd3bc244656b26b939ae771e0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6272/34016 [====>.........................] - ETA: 1:26"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_status=True\n",
    "train_status=False\n",
    "test_status=False\n",
    "# test_status=True\n",
    "\n",
    "model_dict={'name':'5','description':' 2conv layers (6), bigger Maxpool size','model':None,'history':None}\n",
    "model_dict5=f_perform_fit(train_x,train_y,train_wts,test_x,test_y,test_wts,model_dict,num_epochs=5,train_status,test_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "name 1\n",
      "description simplest\n",
      "model <keras.engine.training.Model object at 0x2aed939d4860>\n",
      "history {'val_loss': [0.3614254333144126, 0.3227415867653143, 0.26999167760410264, 0.24859269623816302, 0.2539909265686087], 'val_acc': [0.8614781601722009, 0.8775722303090896, 0.9077411883516705, 0.9123140422282034, 0.9008819075368711], 'loss': [0.5055176187268231, 0.33989128664735485, 0.28450851341929856, 0.26161940508354103, 0.2418150477575659], 'acc': [0.8289529492653417, 0.8804645108466677, 0.9014815789867288, 0.9083848887736271, 0.913811007269823]}\n",
      "Model 2\n",
      "name 2\n",
      "description simplest\n",
      "model <keras.engine.training.Model object at 0x2aed9dbbb208>\n",
      "history {'val_loss': [0.48737753331298395, 0.48636280836439416, 0.4682738286653423, 0.4410965909961992, 0.4180907299727999], 'val_acc': [0.8058615672452775, 0.8063663628030766, 0.807672892482086, 0.8114737060937497, 0.81756094664368], 'loss': [1.2012339293326477, 0.4733139974947547, 0.4456613794283641, 0.41768144707070703, 0.3936835106274213], 'acc': [0.7939098767092042, 0.8138885232518395, 0.8225615374524801, 0.8340573033275889, 0.846430608576175]}\n",
      "Model 3\n",
      "name 3\n",
      "description 6conv size, extra dropout\n",
      "model <keras.engine.training.Model object at 0x2aed9f4802e8>\n",
      "history {'val_loss': [0.4903202474827006, 0.4827211840387357, 0.47584990194189536, 0.460243709415262, 0.44246620031572964], 'val_acc': [0.8056834041072307, 0.8059506488143008, 0.8065445259411232, 0.8088606467357309, 0.8134335006122638], 'loss': [1.3657079573216218, 0.4801899714546136, 0.4655576507138883, 0.44379745927778336, 0.40672634203463315], 'acc': [0.7929445833925384, 0.8090474310086968, 0.8130109838713881, 0.8212306027259125, 0.8360025156146317]}\n",
      "Model 4\n",
      "name 4\n",
      "description 6conv size, extra dropout\n",
      "model <keras.engine.training.Model object at 0x2aeda5ebc438>\n",
      "history {'val_loss': [0.49258574351111284, 0.47718354785467487, 0.3991241869816861, 0.3577449634601743, 0.3495076445835937], 'val_acc': [0.8057130979635718, 0.8064554443720999, 0.8148588057166377, 0.8298245093125638, 0.83157644683669], 'loss': [1.3012402931945928, 0.48118023888380945, 0.44200365965304605, 0.3957410364335779, 0.37349915764064817], 'acc': [0.7950068009310635, 0.8101589808903695, 0.8204408172842907, 0.8365144135869249, 0.8463574802945434]}\n",
      "Model 5\n",
      "name 5\n",
      "description  2conv layers (6), bigger Maxpool size\n",
      "model <keras.engine.training.Model object at 0x2aeda769f518>\n",
      "history {'val_loss': [2.9154385310354156, 0.4864067227629736, 0.4835092914013489, 0.48096064802557986, 0.48116785526792444], 'val_acc': [0.8065445259411232, 0.8069008522172167, 0.8066633013664878, 0.8061288119523475, 0.8060694242396653], 'loss': [3.021343484630045, 0.9137475573373912, 0.4873022315581427, 0.48533001936169146, 0.4801181733020682], 'acc': [0.8054641452087465, 0.8042648413882442, 0.8084039021294667, 0.8084770304110983, 0.8091644362566922]}\n"
     ]
    }
   ],
   "source": [
    "### Comparing different models:\n",
    "\n",
    "for num,md in enumerate([model_dict1,model_dict2,model_dict3,model_dict4,model_dict5]):\n",
    "    hist=md\n",
    "#     print(md)\n",
    "    print('Model %s'%(num+1))\n",
    "    for key in hist.keys():\n",
    "        print(key,hist[key])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-plot\n",
    "# m=model_dict1\n",
    "# f_plot_learning(m['history'])\n",
    "# f_test_model(test_x,test_y,m['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Why are fpr and tpr different for 2 different models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- model.fit \n",
    "    - batch_size= sample of data used for training (subset of full training set). \n",
    "    - epoch= number of runs over training data\n",
    "    - callbacks=\n",
    "    \n",
    "- for layers.Input need size (x,y,z,1) in channels_last mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roc curve notes:\n",
    "- We know y-value depending on signal or background (0 or 1).\n",
    "- The 3D-Cnn gives us a prediction for y, as a float between 0 or 1.\n",
    "- We must use a cut (threshold) to determine what constitues 0 / 1. Eg. 0.5\n",
    "- This gives us a false +ve rate a, true +ve .(fpr and tpr)\n",
    "- Roc curve plots this when varying the threshold\n",
    "- AUC gives area under this curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102050,) (34016,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886098b764ad4c8384e9d35e7b34eb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee33c02ed984745abd2f12f756fb919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting weights\n",
    "print(train_wts.shape,test_wts.shape)\n",
    "\n",
    "# Train data \n",
    "plt.figure()\n",
    "plt.plot(train_wts)\n",
    "plt.title(\"train + cv data weigts \")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(test_wts)\n",
    "plt.title(\"test data weights\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "- use weights to split dataset into train- test\n",
    "- plot on log scale\n",
    "- pick the best model\n",
    "- test on reserve data set\n",
    "- running with multiple cores on a batch node.\n",
    "    - ensure you're running inside your conda environment. \n",
    "- using multiple nodes\n",
    "- using GPU nodes\n",
    "- Test a host of models using ipyparallel\n",
    "- make changes to incorporate regular data in training and reserved data in testing\n",
    "- way to store tested values for easy plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
