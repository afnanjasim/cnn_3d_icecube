{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to extract training and testing data from hdf5 files and storing them in the right form in .npy files\n",
    "\n",
    "This script gives processed data reading.\n",
    "Only dependency is util.py\n",
    "- Nov 12, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules from other files\n",
    "from util import add_pulse_to_inp_tensor, get_nonempty_pulses, total_doms, total_height, total_width, get_pulse_array, get_nonempty_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules to make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dataset(filename, sig_or_bg,cut=None):\n",
    "    '''\n",
    "    Create arrays for xinput, yinput and weights from a single file name\n",
    "    '''\n",
    "    ####### Modified by Venkitesh, Nov 19, 2018. \n",
    "    \n",
    "    hf = h5py.File(filename,'r')\n",
    "    \n",
    "    pulse_array_keys = get_nonempty_pulses(hf)\n",
    "    event_array_keys=get_nonempty_events(hf)\n",
    "    # Checking whether the event_array_keys and pulse_array_keys are in order and identical\n",
    "    assert len(pulse_array_keys)==len(event_array_keys), \"Pulse and event array keys have different sizes\"\n",
    "    assert np.array_equal(pulse_array_keys,event_array_keys), \"Pulse array %s and Event array %s are not identical. Possibility of mismatch\"%(pulse_array_keys,event_array_keys)\n",
    "    \n",
    "    if (sig_or_bg=='sig' and cut=='hesse'):\n",
    "        key_lst=[] # List that will store the events that satisfy the cuts.\n",
    "        for evt in event_array_keys:\n",
    "            val=hf['events'][evt]\n",
    "            if ((np.abs(val['true_x'][0])<500) and (np.abs(val['true_y'][0])<500) and (np.abs(val['true_z'][0])<500)) :\n",
    "                print(\"Hesse-cut\",filename)\n",
    "                print(val,val['true_x'][0],val['true_y'][0],val['true_z'][0])\n",
    "                key_lst.append(evt)\n",
    "        array_keys=np.array(key_lst)\n",
    "    else: \n",
    "        array_keys=event_array_keys.copy()\n",
    "    \n",
    "    num_events = len(array_keys)\n",
    "    # Computing the weights\n",
    "    wgts=np.array([hf['events'][event_key]['weight'][0] for event_key in array_keys])\n",
    "            \n",
    "    tens = np.zeros((num_events, total_doms, total_height, total_width))\n",
    "    for ex_num, pulse_array_key in enumerate(array_keys):\n",
    "        pulse_array = get_pulse_array(hf, pulse_array_key)\n",
    "        add_pulse_to_inp_tensor(tens, ex_num, pulse_array)\n",
    "        \n",
    "    lbls = np.ones((num_events,)) if sig_or_bg == \"sig\" else np.zeros((num_events,))\n",
    "        \n",
    "    return tens, lbls, wgts\n",
    "\n",
    "\n",
    "def get_data(filename_list,file_type,cut):\n",
    "    ''' file_type=\"sig\" or \"bg\" '''\n",
    "    \n",
    "    assert (file_type==\"sig\" or file_type==\"bg\"), \"invalid file_type %s: must be sig or bg\"%(file_type)\n",
    "    # Create first row of numpy array\n",
    "    x, y, wt = make_dataset(filename_list[0], file_type)\n",
    "    # Then append to it\n",
    "    for fn in filename_list[1:]:\n",
    "        xs,ys,wts = make_dataset(fn, file_type,cut)\n",
    "        x = np.vstack((x,xs))\n",
    "        y = np.concatenate((y,ys))\n",
    "        wt = np.concatenate((wt,wts))\n",
    "    \n",
    "    return x,y,wt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_get_file_lists(data_folder,mode):\n",
    "    ''' Function to the get the list of signal files and background files (sigpath and bgpath) for reserved and training data. \n",
    "        mode='quick' picks a smaller set of files for quick training. These files have the form '*00.hdf5'.\n",
    "        \n",
    "        Arguments:\n",
    "        data_folder='regular' or 'reserved'\n",
    "        mode='regular' or 'quick'\n",
    "    '''\n",
    "    \n",
    "    if data_folder=='reserved':\n",
    "        sigpath = \"/project/projectdirs/dasrepo/icecube_data/reserved_data/filtered/nugen/11374/clsim-base-4.0.3.0.99_eff/\"\n",
    "        bgpath = \"/global/project/projectdirs/dasrepo/icecube_data/reserved_data/filtered/corsika/11057/\"\n",
    "    else:\n",
    "        sigpath = \"/project/projectdirs/dasrepo/icecube_data/hdf5_out/filtered/nugen/11374/clsim-base-4.0.3.0.99_eff/\"\n",
    "        bgpath = \"/project/projectdirs/dasrepo/icecube_data/hdf5_out/filtered/corsika/11057/\"\n",
    "        \n",
    "    \n",
    "    # For quick testing, use only the file starting with a '00' at the end ('*00.hdf5'). This give a much smaller set of files, for quick testing.\n",
    "    suffix='*00.hdf5' if mode=='quick' else '*.hdf5'     \n",
    "    sig_list=glob.glob(sigpath+suffix)\n",
    "    bg_list=glob.glob(bgpath+suffix)\n",
    "    \n",
    "    return sig_list,bg_list\n",
    "\n",
    "\n",
    "def f_extract_data(data_folder,save_location,mode='normal',cut=None):\n",
    "    '''\n",
    "    Function to perform :\n",
    "    - Data read\n",
    "    - Data format\n",
    "    - Data save to file\n",
    "    \n",
    "    Arguments:\n",
    "    data_folder='regular' or 'reserved'\n",
    "    save_location= location to save the data files (that are very large)\n",
    "    mode='normal' or 'quick'\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def f_concat_temp_files():\n",
    "        ''' get data from temp files, stack numpy array and delete temp files'''\n",
    "        for i in np.arange(count):\n",
    "            prefix='temp_data_%s'%(i)\n",
    "            f1,f2,f3=[prefix+i+'.npy' for i in ['_x','_y','_wts']]\n",
    "            xs,ys,wts=np.load(save_location+f1),np.load(save_location+f2),np.load(save_location+f3)\n",
    "\n",
    "            if i==0:\n",
    "                x=xs;y=ys;wt=wts\n",
    "            else:\n",
    "                x = np.vstack((x,xs))\n",
    "                y = np.concatenate((y,ys))\n",
    "                wt = np.concatenate((wt,wts))\n",
    "\n",
    "            for fname in [f1,f2,f3]: os.remove(save_location+fname) # Delete temp file\n",
    "        return x,y,wt\n",
    "    \n",
    "    \n",
    "    print(\"Type of data:\\t\",data_folder)\n",
    "    \n",
    "    ##########################################\n",
    "    ### Read Data from files ###\n",
    "    sig_list,bg_list=f_get_file_lists(data_folder,mode)\n",
    "    print(len(sig_list),len(bg_list))\n",
    "    \n",
    "    count=0 # counter for index of temp file\n",
    "    for file_list,sig_or_bg in zip([sig_list,bg_list],['sig','bg']):\n",
    "        print('Type: ',sig_or_bg)\n",
    "        num_files=len(file_list); block_size=100\n",
    "        num_blocks=int(num_files/block_size)+1\n",
    "        print(\"Number of blocks\",num_blocks)\n",
    "        for i in np.arange(num_blocks):\n",
    "            t1=time.time()\n",
    "            start=i*block_size\n",
    "            end=None if i==(num_blocks-1) else (i+1)*block_size # exception handling for last block\n",
    "            \n",
    "            f_list=file_list[start:end]\n",
    "            inx,inpy,wts = get_data(f_list,sig_or_bg,cut)\n",
    "            \n",
    "            ### Save data for each block to temp files ###\n",
    "            prefix='temp_data_%s'%(count)\n",
    "            f1,f2,f3=prefix+'_x',prefix+'_y',prefix+'_wts'\n",
    "            for fname,data in zip([f1,f2,f3],[inx,inpy,wts]):\n",
    "                np.save(save_location+fname,data)\n",
    "            \n",
    "            count+=1 # count is updated for both signal and bgnd\n",
    "            t2=time.time()\n",
    "            print(\"block number: \",i,\"Start-End\",start,end,\"  time taken in seconds: \",t2-t1)\n",
    "        \n",
    "        print(\"Number of samples after %s: %s \"%(sig_or_bg,inpy.shape[0]))\n",
    "    print(\"Number of temp files written\",count)\n",
    "    \n",
    "    # concatenate files to get full input data files\n",
    "    t1=time.time()\n",
    "    inx,inpy,wts=f_concat_temp_files()\n",
    "    t2=time.time()\n",
    "    print(\"Time taken for concatenating temp files\",t2-t1)\n",
    "    num=inx.shape[0]\n",
    "    print(\"Data shape after read:\\tx:{0}\\ty:{1}\\twts:{2}\".format(inx.shape,inpy.shape,wts.shape))\n",
    "    \n",
    "    ##########################################\n",
    "    ### Format the x-data for keras 3D CNN ###\n",
    "    inx2=np.expand_dims(inx,axis=1)\n",
    "    inx3=np.transpose(inx2,axes=[0,3,4,2,1])\n",
    "    # print(inx2.shape,inx3.shape)\n",
    "    inpx=inx3.copy()\n",
    "    print(\"Data shape after format:\\tx:{0}\\ty:{1}\".format(inpx.shape,inpy.shape,wts.shape))\n",
    "    \n",
    "    ##########################################\n",
    "    ### Save data to files ###\n",
    "    prefix='processed_input_'+data_folder\n",
    "    f1,f2,f3=prefix+'_x',prefix+'_y',prefix+'_wts'\n",
    "\n",
    "    for fname,data in zip([f1,f2,f3],[inpx,inpy,wts]):\n",
    "        np.save(save_location+fname,data)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in minutes  7.947285970052083e-09\n",
      "Type of data:\t reserved\n",
      "72 574\n",
      "Type:  sig\n",
      "Number of blocks 1\n",
      "block number:  0 Start-End 0 None   time taken in seconds:  1.7214536666870117\n",
      "Number of samples after sig: 2 \n",
      "Type:  bg\n",
      "Number of blocks 6\n",
      "block number:  0 Start-End 0 100   time taken in seconds:  100.10416007041931\n",
      "block number:  1 Start-End 100 200   time taken in seconds:  97.42812776565552\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    save_data_dir='/global/project/projectdirs/dasrepo/vpa/ice_cube/data_for_cnn/extracted_data_v/data/'\n",
    "    ### Regular data\n",
    "    t1=time.time()\n",
    "#     f_extract_data(data_folder='regular',save_location=save_data_dir,mode='quick',cut='hesse')\n",
    "    f_extract_data(data_folder='regular',save_location=save_data_dir,mode='normal',cut='hesse')\n",
    "\n",
    "    t2=time.time()\n",
    "    print(\"Time taken in minutes \",(t2-t1)/60.0)\n",
    "\n",
    "    ### Reserved data ###\n",
    "    t1=time.time()\n",
    "#     f_extract_data(data_folder='reserved',save_location=save_data_dir,mode='quick',cut='hesse')\n",
    "    f_extract_data(data_folder='reserved',save_location=save_data_dir,mode='normal',cut='hesse')\n",
    "    t2=time.time()\n",
    "    \n",
    "    print(\"Time taken in minutes \",(t2-t1)/60.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook hesse_filter_extract_data.ipynb to script\n",
      "[NbConvertApp] Writing 8924 bytes to hesse_filter_extract_data.py\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to script hesse_filter_extract_data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "Nov 12, 2018\n",
    "\n",
    "Tested this code by doing a diff of regular files with those produced before and they match!\n",
    "Test of times for various stages:\n",
    "\n",
    "#### Regular data:\n",
    "Rough times for each stage in seconds\n",
    "Time for signal      |    2260\n",
    "Time for bg          |  8320\n",
    "Time for extraction  |  323\n",
    "\n",
    "Total time in hours:    2.91 hours\n",
    "\n",
    "#### Reserved data:\n",
    "\n",
    "Rough times for each stage in seconds\n",
    "Time for signal       ||  8588\n",
    "Time for bg           ||  51300\n",
    "Time for extraction   ||  10803\n",
    "\n",
    "Total time in hours:    18.45 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
