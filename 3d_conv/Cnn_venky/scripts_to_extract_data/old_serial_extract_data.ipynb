{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to extract training and testing data from hdf5 files and storing them in the right form in .npy files\n",
    "\n",
    "This script gives processed data reading.\n",
    "Only dependency is util.py\n",
    "- Oct 25, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules from other files\n",
    "\n",
    "from util import add_pulse_to_inp_tensor, get_nonempty_pulses, total_doms, total_height, total_width, get_pulse_array, get_nonempty_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules to make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dataset(filename, sig_or_bg):\n",
    "    \n",
    "    hf = h5py.File(filename,'r')\n",
    "    pulse_array_keys = get_nonempty_pulses(hf)\n",
    "    num_events = len(pulse_array_keys)\n",
    "    \n",
    "    ####### Added by Venkitesh, Oct 24, 2018. \n",
    "    ## Extracting the weights\n",
    "    event_array_keys=get_nonempty_events(hf)\n",
    "    assert len(pulse_array_keys)==len(event_array_keys), \"Pulse and event array keys have different sizes\"\n",
    "    # Computing the weights\n",
    "    wgts=np.array([hf['events'][event_key]['weight'][0] for event_key in event_array_keys])\n",
    "    \n",
    "    # Checking whether the event_array_keys and pulse_array_keys are in order and identical\n",
    "#     print(np.array_equal(pulse_array_keys,event_array_keys))    \n",
    "    assert np.array_equal(pulse_array_keys,event_array_keys), \"Pulse array %s and Event array %s are not identical. Possibility of mismatch\"%(pulse_array_keys,event_array_keys)\n",
    "    #######\n",
    "        \n",
    "    tens = np.zeros((num_events, total_doms, total_height, total_width))\n",
    "    \n",
    "    for ex_num, pulse_array_key in enumerate(pulse_array_keys):\n",
    "        pulse_array = get_pulse_array(hf, pulse_array_key)\n",
    "        add_pulse_to_inp_tensor(tens, ex_num, pulse_array)\n",
    "        \n",
    "    lbls = np.ones((num_events,)) if sig_or_bg == \"sig\" else np.zeros((num_events,))\n",
    "        \n",
    "    return tens, lbls, wgts\n",
    "\n",
    "\n",
    "def get_data(sig_filename_list, bg_filename_list):\n",
    "    \n",
    "    ### Changes made by Venkitesh, 10/24/2018\n",
    "    x, y, wt = make_dataset(sig_filename_list[0], \"sig\")\n",
    "    \n",
    "    for fn in sig_filename_list[1:]:\n",
    "    #for fn in sig_filename_list:\n",
    "        xs,ys,wts = make_dataset(fn, \"sig\")\n",
    "        x = np.vstack((x,xs))\n",
    "        y = np.concatenate((y,ys))\n",
    "        wt = np.concatenate((wt,wts))\n",
    "\n",
    "    for fn in bg_filename_list:\n",
    "        xb,yb,wtb = make_dataset(fn, \"bg\")\n",
    "        x = np.vstack((x,xb))\n",
    "        y = np.concatenate((y,yb))\n",
    "        wt= np.concatenate((wt,wtb))\n",
    "        \n",
    "        \n",
    "    return x,y,wt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_get_file_lists(data_folder,mode):\n",
    "    ''' Function to the get the list of signal files and background files (sigpath and bgpath) for reserved and training data. \n",
    "        mode='quick' picks a smaller set of files for quick training. These files have the form '*00.hdf5'.\n",
    "        \n",
    "        Arguments:\n",
    "        data_folder='regular' or 'reserved'\n",
    "        mode='regular' or 'quick'\n",
    "    '''\n",
    "    \n",
    "    if data_folder=='reserved':\n",
    "        sigpath = \"/project/projectdirs/dasrepo/icecube_data/reserved_data/filtered/nugen/11374/clsim-base-4.0.3.0.99_eff/\"\n",
    "        bgpath = \"/global/project/projectdirs/dasrepo/icecube_data/reserved_data/filtered/corsika/11057/\"\n",
    "    else:\n",
    "        sigpath = \"/project/projectdirs/dasrepo/icecube_data/hdf5_out/filtered/nugen/11374/clsim-base-4.0.3.0.99_eff/\"\n",
    "        bgpath = \"/project/projectdirs/dasrepo/icecube_data/hdf5_out/filtered/corsika/11057/\"\n",
    "        \n",
    "    \n",
    "    # For quick testing, use only the file starting with a '00' at the end ('*00.hdf5'). This give a much smaller set of files, for quick testing.\n",
    "    suffix='*00.hdf5' if mode=='quick' else '*.hdf5'     \n",
    "    sig_list=glob.glob(sigpath+suffix)\n",
    "    bg_list=glob.glob(bgpath+suffix)\n",
    "    \n",
    "    return sig_list,bg_list\n",
    "\n",
    "\n",
    "def f_extract_data(data_folder,save_location,mode='normal'):\n",
    "    '''\n",
    "    Function to perform :\n",
    "    - Data read\n",
    "    - Data format\n",
    "    - Data save to file\n",
    "    \n",
    "    Arguments:\n",
    "    data_folder='regular' or 'reserved'\n",
    "    save_location= location to save the data files (that are very large)\n",
    "    mode='normal' or 'quick'\n",
    "    '''\n",
    "    \n",
    "    print(\"Type of data:\\t\",data_folder)\n",
    "    \n",
    "    ##########################################\n",
    "    ### Read Data from files ###\n",
    "    sig_list,bg_list=f_get_file_lists(data_folder,mode)\n",
    "    print(len(sig_list),len(bg_list))\n",
    "    \n",
    "    return\n",
    "    \n",
    "#     block_size=length/\n",
    "    \n",
    "    \n",
    "    \n",
    "    inx,inpy,wts = get_data(sig_list, bg_list)\n",
    "    num=inx.shape[0]\n",
    "    print(\"Data shape after read:\\tx:{0}\\ty:{1}\\twts:{2}\".format(inx.shape,inpy.shape,wts.shape))\n",
    "    \n",
    "    ##########################################\n",
    "    ### Format the x-data for keras 3D CNN ###\n",
    "    inx2=np.expand_dims(inx,axis=1)\n",
    "    inx3=np.transpose(inx2,axes=[0,3,4,2,1])\n",
    "    # print(inx2.shape,inx3.shape)\n",
    "    inpx=inx3.copy()\n",
    "    print(\"Data shape after format:\\tx:{0}\\ty:{1}\".format(inpx.shape,inpy.shape,wts.shape))\n",
    "    \n",
    "    ##########################################\n",
    "    ### Save data to files ###\n",
    "    prefix='processed_input_'+data_folder\n",
    "    f1,f2,f3=prefix+'_x',prefix+'_y',prefix+'_wts'\n",
    "\n",
    "#     for fname,data in zip([f1,f2,f3],[inpx,inpy,wts]):\n",
    "#         np.save(save_location+fname,data)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of data:\t regular\n",
      "11361 10384\n",
      "Time taken in hours  0.00021515283319685193\n",
      "Type of data:\t reserved\n",
      "7593 57635\n",
      "Time taken in hours  0.0005267043246163262\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    save_data_dir='/global/project/projectdirs/dasrepo/vpa/ice_cube/data_for_cnn/extracted_data_v/data/temp/'\n",
    "#     # Regular data\n",
    "    t1=time.time()\n",
    "#     f_extract_data(data_folder='regular',save_location=save_data_dir,mode='quick')\n",
    "    f_extract_data(data_folder='regular',save_location=save_data_dir,mode='normal')\n",
    "\n",
    "    t2=time.time()\n",
    "    print(\"Time taken in hours \",(t2-t1)/3600.0)\n",
    "\n",
    "    ### Reserved data ###\n",
    "    t1=time.time()\n",
    "#     f_extract_data(data_folder='reserved',save_location=save_data_dir,mode='quick')\n",
    "    f_extract_data(data_folder='reserved',save_location=save_data_dir,mode='normal')\n",
    "    t2=time.time()\n",
    "\n",
    "    print(\"Time taken in hours \",(t2-t1)/3600.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbconvert --to script extract_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 128954\n",
      "drwxrwx--- 2 vpa vpa      512 Oct 25 17:03 \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
      "-rw-r--r-- 1 vpa vpa 50000000 Nov  8 10:33 array_a.txt\n",
      "-rw-r--r-- 1 vpa vpa 50000000 Nov  8 10:33 array_b.txt\n",
      "-rw-r--r-- 1 vpa vpa 16000128 Nov  8 10:33 bin_a.txt.npy\n",
      "-rw-r--r-- 1 vpa vpa 16000128 Nov  8 10:33 bin_b.txt.npy\n",
      "-rw-r--r-- 1 vpa vpa     9967 Nov 11 07:31 extract_data.ipynb\n",
      "-rw-r--r-- 1 vpa vpa     5723 Nov 11 07:30 extract_data.py\n",
      "-rw-r----- 1 vpa vpa     2231 Oct 25 11:34 load_data.pyc\n",
      "drwxrwx--- 4 vpa vpa      512 Oct 25 11:34 \u001b[01;34mold_extract_files\u001b[0m/\n",
      "-rw-r----- 1 vpa vpa     4158 Oct 25 16:14 util.py\n"
     ]
    }
   ],
   "source": [
    "ls -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
