{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Ice-Cube 3D CNN\n",
    "\n",
    "- Oct 29, 2018: This code just makes plots for previously trained CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful blog for keras conv3D: http://learnandshare645.blogspot.com/2016/06/3d-cnn-in-keras-action-recognition.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# keras modules\n",
    "import keras\n",
    "from keras import layers, models, optimizers, callbacks  # or tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "- Data processing\n",
    "    - Read raw data\n",
    "    - Process data\n",
    "    - Save process data\n",
    "    - Read processed data\n",
    "- Model\n",
    "    - Define model\n",
    "    - Train model\n",
    "    - Validate model\n",
    "    - Plot accuracy and loss\n",
    "    - Save model\n",
    "- Test\n",
    "    - Read model\n",
    "    - Read training data\n",
    "    - Get weights\n",
    "    - Test model\n",
    "    - Plot ROC curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from files\n",
    "def f_load_data(data_dir,f1,f2,f3):\n",
    "    ''' Load extracted data from files. Three files for xdata,ydata,weights.\n",
    "    arguments: data directory, f1,f2,f3 \n",
    "    returns : inpx,inpy,weights as arrays\n",
    "    '''\n",
    "\n",
    "    inpx=np.load(data_dir+f1+'.npy')\n",
    "    inpy=np.load(data_dir+f2+'.npy')\n",
    "    wts=np.load(data_dir+f3+'.npy')\n",
    "    print(inpx.shape,inpy.shape)\n",
    "    \n",
    "    \n",
    "    return inpx,inpy,wts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Format data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Shuffle and split data ####\n",
    "\n",
    "def f_shuffle_data(inpx,inpy,wts):\n",
    "    ## Shuffle data\n",
    "    \n",
    "    # Setting seed\n",
    "    seed=243\n",
    "    np.random.seed(seed=seed)\n",
    "\n",
    "    ## Get shuffled array of indices\n",
    "    shuffle_arr=np.arange(inpx.shape[0])\n",
    "    np.random.shuffle(shuffle_arr)\n",
    "    inpx=inpx[shuffle_arr]\n",
    "    inpy=inpy[shuffle_arr]\n",
    "    wts=wts[shuffle_arr]\n",
    "\n",
    "    return inpx,inpy,wts\n",
    "\n",
    "def f_drop_data(inpx,inpy,wts,data_size):\n",
    "    # Drop data for quick training. Just taking the slice of the data from the top.\n",
    "    \n",
    "    full_size=inpy.shape[0]\n",
    "    assert(data_size<=full_size),\"data_size: %s in f_drop_data is more than full data size: %s\"%(data_size,full_size)\n",
    "        \n",
    "    temp=inpx[:data_size]\n",
    "    del(inpx)\n",
    "    inpx=temp.copy()\n",
    "    temp=inpy[:data_size]\n",
    "    del(inpy)\n",
    "    inpy=temp.copy()\n",
    "    temp=wts[:data_size]\n",
    "    del(wts)\n",
    "    wts=temp.copy()\n",
    "    \n",
    "    del(temp)    \n",
    "    \n",
    "    \n",
    "    return (inpx,inpy,wts)        \n",
    "        \n",
    "       \n",
    "\n",
    "def f_split_data(inpx,inpy,wts,test_fraction):\n",
    "    '''\n",
    "    Split data for training and test. validation from training piece of data.\n",
    "    !! Warning this code deletes inpx,inpy inside the function. can't help it because the arrays are too big!!\n",
    "    '''\n",
    "    \n",
    "    num=inpx.shape[0]\n",
    "    test_idx=int(test_fraction*num)\n",
    "    train_idx=num-test_idx\n",
    "\n",
    "    train_x=inpx[:train_idx]\n",
    "    train_y=inpy[:train_idx]\n",
    "    train_wts=wts[:train_idx]\n",
    "    \n",
    "    test_x=inpx[train_idx:]\n",
    "    test_y=inpy[train_idx:]\n",
    "    test_wts=wts[train_idx:]\n",
    "    \n",
    "    return train_x,train_y,train_wts,test_x,test_y,test_wts\n",
    "\n",
    "\n",
    "def f_format_data(inpx,inpy,wts,shuffle_flag=True,drop_data=True,data_size=1000,test_fraction=0.25):\n",
    "    ''' Shuffle, drop and split data for train-test\n",
    "    '''\n",
    "    # Shuffle data\n",
    "    if shuffle_flag: inpx,inpy,wts=f_shuffle_data(inpx,inpy,wts)\n",
    "    # Drop data\n",
    "    if drop_data: inpx,inpy,wts=f_drop_data(inpx,inpy,wts,data_size)\n",
    "\n",
    "#     print(inpy[inpy==0.0].shape,inpy[inpy>0.0].shape,inpy.shape)\n",
    "    \n",
    "#     # Plot data\n",
    "#     plt.figure()\n",
    "#     plt.plot(inpy[:],linestyle='',marker='*',markersize=1)\n",
    "#     plt.title(\"Plot of y data after shuffle\")\n",
    "#     plt.show() \n",
    "    \n",
    "    # Split data into train-test.\n",
    "    train_x,train_y,train_wts,test_x,test_y,test_wts=f_split_data(inpx,inpy,wts,test_fraction)\n",
    "    \n",
    "    print('Data sizes: train_x{0},train_y{1},test_x{2},test_y{3}'.format(train_x.shape,train_y.shape,test_x.shape,test_y.shape))\n",
    "\n",
    "    return train_x,train_y,train_wts,test_x,test_y,test_wts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining all the models tried in the study\n",
    "\n",
    "def f_define_model(inpx,name):\n",
    "    '''\n",
    "    Function that defines the model and compiles it.\n",
    "    '''\n",
    "    \n",
    "    inputs = layers.Input(shape=inpx.shape[1:])\n",
    "    h = inputs\n",
    "    \n",
    "    # Choose model\n",
    "    if name=='1':\n",
    "        print(\"model %s\"%name)\n",
    "        # Convolutional layers\n",
    "        conv_sizes=[10, 10, 10]\n",
    "        conv_args = dict(kernel_size=(3, 3, 3), activation='relu', padding='same')\n",
    "        for conv_size in conv_sizes:\n",
    "            h = layers.Conv3D(conv_size, **conv_args)(h)\n",
    "            h = layers.MaxPooling3D(pool_size=(2, 2, 2))(h)\n",
    "    #         h = layers.Dropout(0.5)(h)\n",
    "        h = layers.Flatten()(h)\n",
    "\n",
    "        # Fully connected  layers\n",
    "        h = layers.Dense(10, activation='relu')(h)\n",
    "        #    h = layers.Dropout(0.5)(h)\n",
    "\n",
    "        # Ouptut layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(h)\n",
    "    \n",
    "    \n",
    "    elif name=='2':\n",
    "        print(\"model %s\"%name)\n",
    "        # Convolutional layers\n",
    "        conv_sizes=[10,10,10]\n",
    "        conv_args = dict(kernel_size=(3, 3, 3), activation='relu', padding='same')\n",
    "        for conv_size in conv_sizes:\n",
    "            h = layers.Conv3D(conv_size, **conv_args)(h)\n",
    "            h = layers.MaxPooling3D(pool_size=(2, 2, 2))(h)\n",
    "            h = layers.Dropout(0.5)(h)\n",
    "        h = layers.Flatten()(h)\n",
    "\n",
    "        # Fully connected  layers\n",
    "        h = layers.Dense(64, activation='relu')(h)\n",
    "        h = layers.Dropout(0.5)(h)\n",
    "\n",
    "        # Ouptut layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(h)\n",
    "        \n",
    "    elif name=='3':\n",
    "        print(\"model %s\"%name)\n",
    "        # Convolutional layers\n",
    "        conv_sizes=[6,6,6]\n",
    "        conv_args = dict(kernel_size=(3, 3, 3), activation='relu', padding='same')\n",
    "        for conv_size in conv_sizes:\n",
    "            h = layers.Conv3D(conv_size, **conv_args)(h)\n",
    "            h = layers.MaxPooling3D(pool_size=(2, 2, 2))(h)\n",
    "            h = layers.Dropout(0.5)(h)\n",
    "        h = layers.Flatten()(h)\n",
    "\n",
    "        # Fully connected  layers\n",
    "        h = layers.Dense(64, activation='relu')(h)\n",
    "        h = layers.Dropout(0.5)(h)\n",
    "\n",
    "        # Ouptut layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(h)\n",
    "    \n",
    "    elif name=='4':\n",
    "        print(\"model %s\"%name)\n",
    "        # Convolutional layers\n",
    "        conv_sizes=[6,6,6]\n",
    "        conv_args = dict(kernel_size=(3, 3, 3), activation='relu', padding='same')\n",
    "        for conv_size in conv_sizes:\n",
    "            h = layers.Conv3D(conv_size, **conv_args)(h)\n",
    "            h = layers.MaxPooling3D(pool_size=(2, 2, 2))(h)\n",
    "            h = layers.Dropout(0.5)(h)\n",
    "        h = layers.Flatten()(h)\n",
    "\n",
    "        # Fully connected  layers\n",
    "        h = layers.Dense(120, activation='relu')(h)\n",
    "        h = layers.Dropout(0.5)(h)\n",
    "\n",
    "        # Ouptut layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(h)\n",
    "        \n",
    "    elif name=='5':\n",
    "        print(\"model %s\"%name)\n",
    "        # Convolutional layers\n",
    "        conv_sizes=[6,6]\n",
    "        conv_args = dict(kernel_size=(2, 4, 15), activation='relu', padding='same')\n",
    "        for conv_size in conv_sizes:\n",
    "            h = layers.Conv3D(conv_size, **conv_args)(h)\n",
    "            h = layers.MaxPooling3D(pool_size=(3, 3, 3))(h)\n",
    "            h = layers.Dropout(0.5)(h)\n",
    "        h = layers.Flatten()(h)\n",
    "\n",
    "        # Fully connected  layers\n",
    "        h = layers.Dense(120, activation='relu')(h)\n",
    "        h = layers.Dropout(0.5)(h)\n",
    "\n",
    "        # Ouptut layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(h)\n",
    "        \n",
    "    \n",
    "    ############################################\n",
    "    ####### Compile model ######################\n",
    "    ############################################\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and perform fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_train_model(model,inpx,inpy):\n",
    "    '''\n",
    "    Train model. Returns just history.history\n",
    "    '''\n",
    "    cv_fraction=0.33 # Fraction of data for cross validation\n",
    "    \n",
    "    history=model.fit(x=inpx, y=inpy,\n",
    "                    batch_size=32,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "#                     callbacks = [callbacks.ModelCheckpoint('./rpv_weights.h5')],\n",
    "                    validation_split=cv_fraction,\n",
    "                    shuffle=True\n",
    "                )\n",
    "    \n",
    "    print(\"Number of parameters\",model.count_params())\n",
    "    \n",
    "    return history.history\n",
    "\n",
    "def f_plot_learning(history):\n",
    "    \n",
    "    plt.figure()\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history['acc'],label='Train')\n",
    "    plt.plot(history['val_acc'],label='Validation')\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure()\n",
    "    plt.plot(history['loss'],label='Train')\n",
    "    plt.plot(history['val_loss'],label='Validation')\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def f_plot_roc_curve(fpr,tpr):\n",
    "    '''\n",
    "    Module for roc plot and printing AUC\n",
    "    '''\n",
    "    plt.figure()\n",
    "    # plt.plot(fpr,tpr)\n",
    "    plt.scatter(fpr,tpr)\n",
    "    plt.xscale('log')\n",
    "    plt.show()\n",
    "\n",
    "    # AUC \n",
    "    auc_val = auc(fpr, tpr)\n",
    "    print(\"AUC: \",auc_val)\n",
    "    \n",
    "\n",
    "def f_test_model(xdata,ydata,wts,model,model_name,model_save_dir,test_status=False):\n",
    "    '''\n",
    "    Test model and make ROC plot\n",
    "    If model has been tested, store the y-predict values\n",
    "    and read them in next time.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    test_file_name=model_save_dir+'y-predict_model-'+str(model_name)+'.pred'\n",
    "    \n",
    "    \n",
    "#     model.evaluate(xdata,ydata,sample_weights=wts,verbose=1)\n",
    "    if not test_status:# Predict values and store to file.\n",
    "        y_pred=model.predict(xdata,verbose=1)\n",
    "        # Save prediction file\n",
    "        np.savetxt(test_file_name,y_pred)\n",
    "        \n",
    "    else: # Load y_predictions from file.\n",
    "        print(\"Using test prediction from previous test\",test_file_name)\n",
    "        y_pred=np.loadtxt(test_file_name)\n",
    "        \n",
    "#     print(y_pred)\n",
    "    fpr,tpr,threshold=roc_curve(ydata,y_pred,sample_weight=wts)\n",
    "    print(fpr.shape,tpr.shape,threshold.shape)\n",
    "    f_plot_roc_curve(fpr,tpr)\n",
    "\n",
    "\n",
    "def f_perform_fit(train_x,train_y,train_wts,test_x,test_y,test_wts,model_dict,train_status=False,test_status=False):\n",
    "    '''\n",
    "    Compile, train, save and test the model.\n",
    "    Steps:\n",
    "    - Compile\n",
    "    - Train\n",
    "    - Save\n",
    "    - Read\n",
    "    - Plot\n",
    "    - Test\n",
    "    \n",
    "    Note: Cross-validation data is built into the training. So, train_{x/y} contains the training and cval data.\n",
    "    '''\n",
    "    \n",
    "    model_save_dir='/global/project/projectdirs/dasrepo/vpa/ice_cube/data_for_cnn/saved_models/'\n",
    "    model_name=model_dict['name'] # string for the model\n",
    "    fname_model,fname_history='model_{0}.h5'.format(model_name),'history_{0}.pickle'.format(model_name)\n",
    "    \n",
    "    if not train_status: # If not trained before, train the model and save it.\n",
    "\n",
    "        ########################\n",
    "        # Compile model\n",
    "        model=f_define_model(train_x,model_name)\n",
    "        # Train model\n",
    "        history=f_train_model(model,train_x,train_y)\n",
    "\n",
    "        ########################\n",
    "        # Save model and history\n",
    "        model.save(model_save_dir+fname_model)\n",
    "        with open(model_save_dir+fname_history, 'wb') as f:\n",
    "                pickle.dump(history, f)\n",
    "    \n",
    "    else:\n",
    "        print(\"Using trained model\")\n",
    "\n",
    "        \n",
    "    ########################\n",
    "    ### Read model and history\n",
    "    \n",
    "    ### Check if files exist\n",
    "    assert os.path.exists(model_save_dir+fname_model),\"Model not saved\"\n",
    "    assert os.path.exists(model_save_dir+fname_history),\"History not saved\"\n",
    "    \n",
    "    model=load_model(model_save_dir+fname_model)\n",
    "    with open(model_save_dir+fname_history,'rb') as f:\n",
    "        history= pickle.load(f)\n",
    "    \n",
    "    ########################\n",
    "    model.summary()\n",
    "    # Plot tested model\n",
    "    f_plot_learning(history)\n",
    "    \n",
    "    ########################\n",
    "    # Test model\n",
    "    f_test_model(test_x,test_y,test_wts,model,model_dict['name'],model_save_dir,test_status)\n",
    "\n",
    "    model_dict['model'],model_dict['history']=model,history\n",
    "    \n",
    "    return model_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_ydata_and_wts(data_dir,f1,f2):\n",
    "    ''' Load extracted data from files. Just extracting ydata and weights\n",
    "    returns : inpx,inpy,weights as arrays\n",
    "    '''\n",
    "\n",
    "    inpy=np.load(data_dir+f1+'.npy')\n",
    "    wts=np.load(data_dir+f1+'.npy')\n",
    "    \n",
    "    return(inpy,wts)\n",
    "    \n",
    "\n",
    "def f_plot_fit(inpy,wts,model_dict):\n",
    "    '''\n",
    "    Plot fit results.\n",
    "    '''\n",
    "    \n",
    "    model_save_dir='/global/project/projectdirs/dasrepo/vpa/ice_cube/data_for_cnn/saved_models/'\n",
    "    model_name=model_dict['name'] # string for the model\n",
    "    fname_model,fname_history='model_{0}.h5'.format(model_name),'history_{0}.pickle'.format(model_name)\n",
    "    \n",
    "        \n",
    "    ########################\n",
    "    ### Read model and history\n",
    "    \n",
    "    ### Check if files exist\n",
    "    assert os.path.exists(model_save_dir+fname_model),\"Model not saved\"\n",
    "    assert os.path.exists(model_save_dir+fname_history),\"History not saved\"\n",
    "    \n",
    "    model=load_model(model_save_dir+fname_model)\n",
    "    with open(model_save_dir+fname_history,'rb') as f:\n",
    "        history= pickle.load(f)\n",
    "    \n",
    "    ########################\n",
    "    model.summary()\n",
    "    # Plot tested model\n",
    "    f_plot_learning(history)\n",
    "    \n",
    "    ########################\n",
    "    # Get test predictions\n",
    "    \n",
    "    test_file_name=model_save_dir+'y-predict_model-'+str(model_name)+'.pred'\n",
    "    assert os.path.exists(model_save_dir+test_file_name),\"y-preditions not saved\"\n",
    "    print(\"Using test prediction from previous test\",test_file_name)\n",
    "    y_pred=np.loadtxt(test_file_name)\n",
    "    assert(test_y.shape==y_pred.shape),\"Data %s and prediction arrays %s are not of the same size\"%(test_y.shape,y_pred.shape)\n",
    "    \n",
    "    fpr,tpr,threshold=roc_curve(ydata,y_pred,sample_weight=wts)\n",
    "    print(fpr.shape,tpr.shape,threshold.shape)\n",
    "    f_plot_roc_curve(fpr,tpr)\n",
    "    \n",
    "    model_dict['model'],model_dict['history']=model,history\n",
    "    \n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def f_format_data(inpy,wts,test_fraction):\n",
    "    \n",
    "    num=inpy.shape[0]\n",
    "    test_idx=int(test_fraction*num)\n",
    "    train_idx=num-test_idx\n",
    "\n",
    "    test_y=inpy[train_idx:]\n",
    "    test_wts=wts[train_idx:]\n",
    "    \n",
    "    return test_y,test_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    ###Extract data : Only extract y-data and weights for tests.\n",
    "    data_dir='/global/project/projectdirs/dasrepo/vpa/ice_cube/data_for_cnn/extracted_data_v/data/'\n",
    "    f1,f2='processed_input_regular_y','processed_input_regular_wts'\n",
    "    inpy,wts=f_get_ydata_and_wts(data_dir,f1,f2)\n",
    "    test_y,test_wts=f_format_data(inpy,wts,test_fraction=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136066,) (136066,) (34016,) (34016,)\n"
     ]
    }
   ],
   "source": [
    "print(inpy.shape,wts.shape,test_y.shape,test_wts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'name': '1', 'description': None, 'model': None, 'history': None}\n"
     ]
    }
   ],
   "source": [
    "model_loc='/global/project/projectdirs/dasrepo/vpa/ice_cube/data_for_cnn/saved_models/'\n",
    "dict_list=[]\n",
    "# for i in range(1,6):\n",
    "for i in range(1,2):\n",
    "    model_dict={'name':str(i),'description':None,'model':None,'history':None}\n",
    "    print(i,model_dict)\n",
    "#     model_dict=f_plot_fit(test_y,test_wts,model_dict)\n",
    "#     dict_list.append(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "('model', <keras.engine.training.Model object at 0x2b6b2716ea50>)\n",
      "('history', {'acc': [0.8359440129893263, 0.8894885408008836, 0.904977110848721, 0.912026677198011, 0.9179646936665], 'loss': [0.4940939987963115, 0.31283381096304425, 0.2737800265921407, 0.24922132658353088, 0.22983019208651087], 'val_acc': [0.8675950945784723, 0.8965762983674083, 0.9038809870255546, 0.9076224129263061, 0.916500875970532], 'val_loss': [0.3586907645015842, 0.287272262804921, 0.2866968094362769, 0.24678741439057064, 0.23340108330942097]})\n",
      "('name', '1')\n",
      "('description', 'simplest')\n",
      "Model 2\n",
      "('model', <keras.engine.training.Model object at 0x2b6b27902750>)\n",
      "('history', {'acc': [0.7970105158495139, 0.8128354759946004, 0.822751670982107, 0.832258347597707, 0.8416187676448132], 'loss': [1.4162641630844435, 0.47639839410129786, 0.44579309258209165, 0.4209624629782049, 0.40285478480902426], 'val_acc': [0.8058615672452775, 0.8069899337862402, 0.8100187071330347, 0.8137898268883573, 0.8116221753754553], 'val_loss': [0.48992596278228984, 0.4785711632370436, 0.460816893320729, 0.4386562026994445, 0.4530109813100642]})\n",
      "('name', '2')\n",
      "('description', 'simplest')\n",
      "Model 3\n",
      "('model', <keras.engine.training.Model object at 0x2b6b281ac5d0>)\n",
      "('history', {'acc': [0.7968203823198869, 0.8111096485489655, 0.8132157430582132, 0.8226492913921816, 0.8312491773111904], 'loss': [1.1826206369144983, 0.47770168102687394, 0.46486923854159573, 0.43820393621523146, 0.41163175838725513], 'val_acc': [0.8056834041072307, 0.8059209549579597, 0.8059209549579597, 0.8095732992879179, 0.8133741128995816], 'val_loss': [0.4916253875152662, 0.48835207324262775, 0.4870096758973922, 0.45401277705068194, 0.4310212045714171]})\n",
      "('name', '3')\n",
      "('description', '6conv size, extra dropout')\n",
      "Model 4\n",
      "('model', <keras.engine.training.Model object at 0x2b6b28b12b90>)\n",
      "('history', {'acc': [0.7934711070211579, 0.8100566012969569, 0.8183347227724278, 0.8383426206294593, 0.8459479619174062], 'loss': [1.118801280050847, 0.47702396273056796, 0.44952836158885034, 0.39217408395042985, 0.37097298751224617], 'val_acc': [0.8057427918199129, 0.8060397303833242, 0.8121863586459367, 0.8308637942845031, 0.820946046266568], 'val_loss': [0.4906789881053105, 0.48344187839943703, 0.426501613036964, 0.3621914132628209, 0.4057584441316649]})\n",
      "('name', '4')\n",
      "('description', '6conv size, extra dropout')\n",
      "Model 5\n",
      "('model', <keras.engine.training.Model object at 0x2b6b293e9690>)\n",
      "('history', {'acc': [0.8053471399598794, 0.808520907377462, 0.8081113890038119, 0.8047474880487565, 0.8073801061892389], 'loss': [3.0675165750531552, 3.073401378076232, 3.088594688898053, 3.1382648834336266, 3.0969896897636113], 'val_acc': [0.8060694242396653, 0.8056834041072307, 0.8056834041072307, 0.8070196276425813, 0.8057130979635718], 'val_loss': [3.1178926803165186, 3.1320134453440263, 3.1320134453440263, 3.0918400076516144, 3.131480504825881]})\n",
      "('name', '5')\n",
      "('description', ' 2conv layers (6), bigger Maxpool size')\n"
     ]
    }
   ],
   "source": [
    "### Comparing different models:\n",
    "\n",
    "for num,md in enumerate([model_dict1,model_dict2,model_dict3,model_dict4,model_dict5]):\n",
    "    hist=md\n",
    "#     print(md)\n",
    "    print('Model %s'%(num+1))\n",
    "    for key in hist.keys():\n",
    "        print(key,hist[key])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-plot\n",
    "# m=model_dict1\n",
    "# f_plot_learning(m['history'])\n",
    "# f_test_model(test_x,test_y,m['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Why are fpr and tpr different for 2 different models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- model.fit \n",
    "    - batch_size= sample of data used for training (subset of full training set). \n",
    "    - epoch= number of runs over training data\n",
    "    - callbacks=\n",
    "    \n",
    "- for layers.Input need size (x,y,z,1) in channels_last mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roc curve notes:\n",
    "- We know y-value depending on signal or background (0 or 1).\n",
    "- The 3D-Cnn gives us a prediction for y, as a float between 0 or 1.\n",
    "- We must use a cut (threshold) to determine what constitues 0 / 1. Eg. 0.5\n",
    "- This gives us a false +ve rate a, true +ve .(fpr and tpr)\n",
    "- Roc curve plots this when varying the threshold\n",
    "- AUC gives area under this curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((102050,), (34016,))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce243ebf9c2430ca1e4bed40fb3c0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c28c0e8d4a4b2d82e33e496352a8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting weights\n",
    "print(train_wts.shape,test_wts.shape)\n",
    "\n",
    "# Train data \n",
    "plt.figure()\n",
    "plt.plot(train_wts)\n",
    "plt.title(\"train + cv data weigts \")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(test_wts)\n",
    "plt.title(\"test data weights\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "- pick the best model\n",
    "- test on reserve data set\n",
    "- running with multiple cores on a batch node.\n",
    "- using multiple nodes\n",
    "- using GPU nodes\n",
    "- Test a host of models using ipyparallel\n",
    "- make changes to incorporate regular data in training and reserved data in testing\n",
    "- way to store tested values for easy plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "- Code to \n",
    "    - show model summary and plots.\n",
    "    - just view existing plots in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
