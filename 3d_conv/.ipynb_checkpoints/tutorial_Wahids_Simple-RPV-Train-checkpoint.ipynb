{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models, optimizers, callbacks  # or tensorflow.keras as keras\n",
    "# from rpv import load_file\n",
    "import os\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename, n_samples):\n",
    "    with h5py.File(filename) as f:\n",
    "        data_group = f['all_events']\n",
    "        data = data_group['hist'][:n_samples][:,:,:,None]\n",
    "        labels = data_group['y'][:n_samples]\n",
    "        weights = data_group['weight'][:n_samples]\n",
    "    return data, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dir = '/global/cscratch1/sd/wbhimji/atlas-rpv-images'\n",
    "input_dir='/global/homes/v/vpa/project/Ice_cube/temp_data/'\n",
    "train_input, train_labels, train_weights = load_file(os.path.join(input_dir, 'train.h5'), 1000)\n",
    "valid_input, valid_labels, valid_weights = load_file(os.path.join(input_dir, 'val.h5'), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64, 64, 1) (1000, 64, 64, 1)\n",
      "(64, 64, 1)\n",
      "(1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape, valid_input.shape)\n",
    "print(train_input.shape[1:])\n",
    "print(train_labels.shape,train_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=train_input.shape[1:])\n",
    "h = inputs\n",
    "\n",
    "# Convolutional layers\n",
    "conv_sizes=[64, 128, 256]\n",
    "conv_args = dict(kernel_size=(3, 3), activation='relu', padding='same')\n",
    "for conv_size in conv_sizes:\n",
    "    h = layers.Conv2D(conv_size, **conv_args)(h)\n",
    "    h = layers.MaxPooling2D(pool_size=(2, 2))(h)\n",
    "#h = layers.Dropout(0.5)(h)\n",
    "h = layers.Flatten()(h)\n",
    "\n",
    "# Fully connected  layers\n",
    "h = layers.Dense(64, activation='relu')(h)\n",
    "#    h = layers.Dropout(0.5)(h)\n",
    "\n",
    "# Ouptut layer\n",
    "outputs = layers.Dense(1, activation='sigmoid')(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(inputs, outputs)\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6734 - acc: 0.5430\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.5149 - acc: 0.5990\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.4035 - acc: 0.8490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b2d89ec62b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input, train_labels,\n",
    "                      batch_size=128,\n",
    "                      epochs=3,\n",
    "                      verbose=1,\n",
    "                      callbacks = [\n",
    "                        callbacks.ModelCheckpoint('./rpv_weights.h5')\n",
    "                      ],                      \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Trains the model for a given number of epochs (iterations on a dataset).\n",
       "\n",
       "# Arguments\n",
       "    x: Numpy array of training data (if the model has a single input),\n",
       "        or list of Numpy arrays (if the model has multiple inputs).\n",
       "        If input layers in the model are named, you can also pass a\n",
       "        dictionary mapping input names to Numpy arrays.\n",
       "        `x` can be `None` (default) if feeding from\n",
       "        framework-native tensors (e.g. TensorFlow data tensors).\n",
       "    y: Numpy array of target (label) data\n",
       "        (if the model has a single output),\n",
       "        or list of Numpy arrays (if the model has multiple outputs).\n",
       "        If output layers in the model are named, you can also pass a\n",
       "        dictionary mapping output names to Numpy arrays.\n",
       "        `y` can be `None` (default) if feeding from\n",
       "        framework-native tensors (e.g. TensorFlow data tensors).\n",
       "    batch_size: Integer or `None`.\n",
       "        Number of samples per gradient update.\n",
       "        If unspecified, `batch_size` will default to 32.\n",
       "    epochs: Integer. Number of epochs to train the model.\n",
       "        An epoch is an iteration over the entire `x` and `y`\n",
       "        data provided.\n",
       "        Note that in conjunction with `initial_epoch`,\n",
       "        `epochs` is to be understood as \"final epoch\".\n",
       "        The model is not trained for a number of iterations\n",
       "        given by `epochs`, but merely until the epoch\n",
       "        of index `epochs` is reached.\n",
       "    verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
       "        0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
       "    callbacks: List of `keras.callbacks.Callback` instances.\n",
       "        List of callbacks to apply during training.\n",
       "        See [callbacks](/callbacks).\n",
       "    validation_split: Float between 0 and 1.\n",
       "        Fraction of the training data to be used as validation data.\n",
       "        The model will set apart this fraction of the training data,\n",
       "        will not train on it, and will evaluate\n",
       "        the loss and any model metrics\n",
       "        on this data at the end of each epoch.\n",
       "        The validation data is selected from the last samples\n",
       "        in the `x` and `y` data provided, before shuffling.\n",
       "    validation_data: tuple `(x_val, y_val)` or tuple\n",
       "        `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
       "        the loss and any model metrics at the end of each epoch.\n",
       "        The model will not be trained on this data.\n",
       "        `validation_data` will override `validation_split`.\n",
       "    shuffle: Boolean (whether to shuffle the training data\n",
       "        before each epoch) or str (for 'batch').\n",
       "        'batch' is a special option for dealing with the\n",
       "        limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
       "        Has no effect when `steps_per_epoch` is not `None`.\n",
       "    class_weight: Optional dictionary mapping class indices (integers)\n",
       "        to a weight (float) value, used for weighting the loss function\n",
       "        (during training only).\n",
       "        This can be useful to tell the model to\n",
       "        \"pay more attention\" to samples from\n",
       "        an under-represented class.\n",
       "    sample_weight: Optional Numpy array of weights for\n",
       "        the training samples, used for weighting the loss function\n",
       "        (during training only). You can either pass a flat (1D)\n",
       "        Numpy array with the same length as the input samples\n",
       "        (1:1 mapping between weights and samples),\n",
       "        or in the case of temporal data,\n",
       "        you can pass a 2D array with shape\n",
       "        `(samples, sequence_length)`,\n",
       "        to apply a different weight to every timestep of every sample.\n",
       "        In this case you should make sure to specify\n",
       "        `sample_weight_mode=\"temporal\"` in `compile()`.\n",
       "    initial_epoch: Integer.\n",
       "        Epoch at which to start training\n",
       "        (useful for resuming a previous training run).\n",
       "    steps_per_epoch: Integer or `None`.\n",
       "        Total number of steps (batches of samples)\n",
       "        before declaring one epoch finished and starting the\n",
       "        next epoch. When training with input tensors such as\n",
       "        TensorFlow data tensors, the default `None` is equal to\n",
       "        the number of samples in your dataset divided by\n",
       "        the batch size, or 1 if that cannot be determined.\n",
       "    validation_steps: Only relevant if `steps_per_epoch`\n",
       "        is specified. Total number of steps (batches of samples)\n",
       "        to validate before stopping.\n",
       "\n",
       "# Returns\n",
       "    A `History` object. Its `History.history` attribute is\n",
       "    a record of training loss values and metrics values\n",
       "    at successive epochs, as well as validation loss values\n",
       "    and validation metrics values (if applicable).\n",
       "\n",
       "# Raises\n",
       "    RuntimeError: If the model was never compiled.\n",
       "    ValueError: In case of mismatch between the provided input data\n",
       "        and what the model expects.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.conda/envs/v_py3/lib/python3.6/site-packages/keras/engine/training.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1048640   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,418,369\n",
      "Trainable params: 1,418,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/step\n",
      "Validation Accuracy: 0.901\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(valid_input,  valid_labels, sample_weight=valid_weights); print (\"Validation Accuracy: \" + str(score[1] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
